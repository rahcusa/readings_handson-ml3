{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd572f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9efc8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ea238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#median = housing[\"total_bedrooms\"].median() # option 3\n",
    "#housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b622c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efacf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c619aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self # always return self!\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5694d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "\"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"] # feature names out\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "log_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "        StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "        StandardScaler())\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "    \"households\", \"median_income\"]),\n",
    "    (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline) # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96dfb8",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d84411",
   "metadata": {},
   "source": [
    "**1) Try a support vector machine regressor (sklearn.svm.SVR) with various\n",
    "hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Note that support vector machines donâ€™t scale well to large\n",
    "datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours.\n",
    "Donâ€™t worry about what the hyperparameters mean for now; weâ€™ll discuss them\n",
    "in Chapter 5. How does the best SVR predictor perform?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# sv_reg = make_pipeline(preprocessing, SVR(kernel=\"linear\"))\n",
    "housing_5000 = housing[0:5000];\n",
    "housing_labels_5000 = housing_labels[0:5000]\n",
    "\n",
    "# def sv_reg_fit_predict_score(k, c=100, g='auto', e=0.1):\n",
    "#     sv_reg = make_pipeline(preprocessing, SVR(kernel=k, C=c, gamma=g, epsilon=e))\n",
    "#     sv_reg.fit(housing_5000, housing_labels_5000)\n",
    "#     housing_predictions_5000 = sv_reg.predict(housing_5000)\n",
    "#     print(housing_predictions_5000[:5].round(-2))\n",
    "#     print(housing_labels_5000.iloc[:5].values)\n",
    "#     sv_rmses = -cross_val_score(sv_reg, housing_5000, housing_labels_5000,\n",
    "#                            scoring=\"neg_root_mean_squared_error\", cv=3)\n",
    "#     print(pd.Series(sv_rmses).describe())\n",
    "\n",
    "# sv_reg_fit_predict(k='linear')\n",
    "# sv_reg_fit_predict(k='rbf', c=100, g=0.1, e=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"svr\", SVR()),\n",
    "])\n",
    "param_grid = [\n",
    "    {'svr__C': [1, 10, 100, 1000, 10000], 'svr__kernel': ['linear']},\n",
    "    {'svr__C': [1.0, 3.0, 10., 30., 100., 300., 1000.0], 'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0], 'svr__kernel': ['rbf']},\n",
    "]\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
    "    scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(housing_5000, housing_labels_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12139ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6efb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "-grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b4d6c",
   "metadata": {},
   "source": [
    "**2) Try replacing GridSearchCV with a RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, loguniform, randint\n",
    "\n",
    "param_distribs = {\n",
    "    'svr__C': loguniform(20, 200_000),\n",
    "    'svr__gamma': expon(scale=1.0),\n",
    "    'svr__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42\n",
    ")\n",
    "rnd_search.fit(housing_5000, housing_labels_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77180d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rnd_search_rmse = -rnd_search.best_score_\n",
    "svr_rnd_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdfd94",
   "metadata": {},
   "source": [
    "**3) Try adding a SelectFromModel transformer in the preparation pipeline to select\n",
    "only the most important attributes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "selector_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),\n",
    "                                 threshold=0.005)),  # min feature importance\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "            gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "            kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])\n",
    "\n",
    "selector_rmses = -cross_val_score(selector_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(selector_rmses).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_param_distribs = {\n",
    "    'selector__threshold': expon(scale=0.1)\n",
    "}\n",
    "\n",
    "rnd_selector_search = RandomizedSearchCV(\n",
    "    selector_pipeline, param_distributions=selector_param_distribs, n_iter=10, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42\n",
    ")\n",
    "rnd_selector_search.fit(housing_5000, housing_labels_5000)\n",
    "print(rnd_selector_search.best_params_)\n",
    "print(-rnd_selector_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80dcff",
   "metadata": {},
   "source": [
    "**4) Try creating a custom transformer that trains a k-nearest neighbors regressor\n",
    "(sklearn.neighbors.KNeighborsRegressor) in its fit() method, and outputs\n",
    "the modelâ€™s predictions in its transform() method. Then add this feature to\n",
    "the preprocessing pipeline, using latitude and longitude as the inputs to this\n",
    "transformer. This will add a feature in the model that corresponds to the housing\n",
    "median price of the nearest districts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e765d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class NearestNeighbors(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_neighbors=5, weights='distance'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kneighbors_ = KNeighborsRegressor(n_neighbors=self.n_neighbors, weights=self.weights)\n",
    "        self.kneighbors_.fit(X, y)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        predictions = self.kneighbors_.predict(X)\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        return predictions\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return 'KNN prediction'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors()\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "    \"households\", \"median_income\"]),\n",
    "    (\"geo\", knn, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline) # one column remaining: housing_median_age\n",
    "\n",
    "new_geo_pipeline = Pipeline([\n",
    "    ('preprocessing', new_geo_preprocessing),\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "                gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "                kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])\n",
    "\n",
    "new_pipe_rmses = -cross_val_score(new_geo_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(new_pipe_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4303134",
   "metadata": {},
   "source": [
    "**6) Try to implement the StandardScalerClone class again from scratch, then add support for the inverse_transform() method: executing scaler.inverse_transform(scaler.fit_transform(X)) should return an array very close to X. \n",
    "Then add support for feature names: \n",
    "set feature_names_in_ in the fit() method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the get_feature_names_out() method: it should have one optional input_features=None argument.\n",
    "If passed, the method should check that its length matches n_features_in_, and it should match feature_names_in_ if it is defined; then input_features should be returned.\n",
    "If input_features is None, then the method should either return feature_names_in_ if it is defined or np.array([\"x0\", \"x1\", ...]) with length n_features_in_ otherwise. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29552dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True): # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None): # y is required even though we don't use it\n",
    "        X = check_array(X) # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1] # every estimator stores this in fit()\n",
    "        if hasattr(X_orig, \"columns\"):\n",
    "            self.feature_names_in_ = np.array(X_orig.columns, dtype=object)\n",
    "        return self # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self) # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        X = X * self.scale_\n",
    "        if self.with_mean:\n",
    "            X = X + self.mean_\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(input_features=None):\n",
    "        if input_features is None:\n",
    "            return getattr(self, \"feature_names_in_\",\n",
    "                           [f\"x{i}\" for i in range(self.n_features_in_)])\n",
    "        else:\n",
    "            if len(input_features) != self.n_features_in_:\n",
    "                raise ValueError(\"Invalid number of features\")\n",
    "            if hasattr(self, \"feature_names_in_\") and not np.all(\n",
    "                self.feature_names_in_ == input_features\n",
    "            ):\n",
    "                raise ValueError(\"input_features â‰  feature_names_in_\")\n",
    "            return input_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
