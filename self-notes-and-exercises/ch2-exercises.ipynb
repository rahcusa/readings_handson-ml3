{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd572f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9efc8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ea238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#median = housing[\"total_bedrooms\"].median() # option 3\n",
    "#housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b622c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efacf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c619aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self # always return self!\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5694d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "\"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"] # feature names out\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "log_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "        StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "        StandardScaler())\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "    \"households\", \"median_income\"]),\n",
    "    (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline) # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96dfb8",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d84411",
   "metadata": {},
   "source": [
    "**1) Try a support vector machine regressor (sklearn.svm.SVR) with various\n",
    "hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Note that support vector machines don’t scale well to large\n",
    "datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours.\n",
    "Don’t worry about what the hyperparameters mean for now; we’ll discuss them\n",
    "in Chapter 5. How does the best SVR predictor perform?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# sv_reg = make_pipeline(preprocessing, SVR(kernel=\"linear\"))\n",
    "housing_5000 = housing[0:5000];\n",
    "housing_labels_5000 = housing_labels[0:5000]\n",
    "\n",
    "# def sv_reg_fit_predict_score(k, c=100, g='auto', e=0.1):\n",
    "#     sv_reg = make_pipeline(preprocessing, SVR(kernel=k, C=c, gamma=g, epsilon=e))\n",
    "#     sv_reg.fit(housing_5000, housing_labels_5000)\n",
    "#     housing_predictions_5000 = sv_reg.predict(housing_5000)\n",
    "#     print(housing_predictions_5000[:5].round(-2))\n",
    "#     print(housing_labels_5000.iloc[:5].values)\n",
    "#     sv_rmses = -cross_val_score(sv_reg, housing_5000, housing_labels_5000,\n",
    "#                            scoring=\"neg_root_mean_squared_error\", cv=3)\n",
    "#     print(pd.Series(sv_rmses).describe())\n",
    "\n",
    "# sv_reg_fit_predict(k='linear')\n",
    "# sv_reg_fit_predict(k='rbf', c=100, g=0.1, e=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"svr\", SVR()),\n",
    "])\n",
    "param_grid = [\n",
    "    {'svr__C': [1, 10, 100, 1000, 10000], 'svr__kernel': ['linear']},\n",
    "    {'svr__C': [1.0, 3.0, 10., 30., 100., 300., 1000.0], 'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0], 'svr__kernel': ['rbf']},\n",
    "]\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
    "    scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(housing_5000, housing_labels_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12139ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6efb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "-grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b4d6c",
   "metadata": {},
   "source": [
    "**2) Try replacing GridSearchCV with a RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, loguniform, randint\n",
    "\n",
    "param_distribs = {\n",
    "    'svr__C': loguniform(20, 200_000),\n",
    "    'svr__gamma': expon(scale=1.0),\n",
    "    'svr__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42\n",
    ")\n",
    "rnd_search.fit(housing_5000, housing_labels_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77180d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rnd_search_rmse = -rnd_search.best_score_\n",
    "svr_rnd_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdfd94",
   "metadata": {},
   "source": [
    "**3) Try adding a SelectFromModel transformer in the preparation pipeline to select\n",
    "only the most important attributes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "selector_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),\n",
    "                                 threshold=0.005)),  # min feature importance\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "            gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "            kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])\n",
    "\n",
    "selector_rmses = -cross_val_score(selector_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(selector_rmses).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_param_distribs = {\n",
    "    'selector__threshold': expon(scale=0.1)\n",
    "}\n",
    "\n",
    "rnd_selector_search = RandomizedSearchCV(\n",
    "    selector_pipeline, param_distributions=selector_param_distribs, n_iter=10, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42\n",
    ")\n",
    "rnd_selector_search.fit(housing_5000, housing_labels_5000)\n",
    "print(rnd_selector_search.best_params_)\n",
    "print(-rnd_selector_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80dcff",
   "metadata": {},
   "source": [
    "**4) Try creating a custom transformer that trains a k-nearest neighbors regressor\n",
    "(sklearn.neighbors.KNeighborsRegressor) in its fit() method, and outputs\n",
    "the model’s predictions in its transform() method. Then add this feature to\n",
    "the preprocessing pipeline, using latitude and longitude as the inputs to this\n",
    "transformer. This will add a feature in the model that corresponds to the housing\n",
    "median price of the nearest districts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e765d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class NearestNeighbors(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_neighbors=5, weights='distance'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kneighbors_ = KNeighborsRegressor(n_neighbors=self.n_neighbors, weights=self.weights)\n",
    "        self.kneighbors_.fit(X, y)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        predictions = self.kneighbors_.predict(X)\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        return predictions\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return 'KNN prediction'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors()\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "    \"households\", \"median_income\"]),\n",
    "    (\"geo\", knn, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline) # one column remaining: housing_median_age\n",
    "\n",
    "new_geo_pipeline = Pipeline([\n",
    "    ('preprocessing', new_geo_preprocessing),\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "                gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "                kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])\n",
    "\n",
    "new_pipe_rmses = -cross_val_score(new_geo_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(new_pipe_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4303134",
   "metadata": {},
   "source": [
    "**6) Try to implement the StandardScalerClone class again from scratch, then add support for the inverse_transform() method: executing scaler.inverse_transform(scaler.fit_transform(X)) should return an array very close to X. \n",
    "Then add support for feature names: \n",
    "set feature_names_in_ in the fit() method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the get_feature_names_out() method: it should have one optional input_features=None argument.\n",
    "If passed, the method should check that its length matches n_features_in_, and it should match feature_names_in_ if it is defined; then input_features should be returned.\n",
    "If input_features is None, then the method should either return feature_names_in_ if it is defined or np.array([\"x0\", \"x1\", ...]) with length n_features_in_ otherwise. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29552dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True): # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None): # y is required even though we don't use it\n",
    "        X = check_array(X) # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1] # every estimator stores this in fit()\n",
    "        if hasattr(X_orig, \"columns\"):\n",
    "            self.feature_names_in_ = np.array(X_orig.columns, dtype=object)\n",
    "        return self # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self) # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        X = X * self.scale_\n",
    "        if self.with_mean:\n",
    "            X = X + self.mean_\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(input_features=None):\n",
    "        if input_features is None:\n",
    "            return getattr(self, \"feature_names_in_\",\n",
    "                           [f\"x{i}\" for i in range(self.n_features_in_)])\n",
    "        else:\n",
    "            if len(input_features) != self.n_features_in_:\n",
    "                raise ValueError(\"Invalid number of features\")\n",
    "            if hasattr(self, \"feature_names_in_\") and not np.all(\n",
    "                self.feature_names_in_ == input_features\n",
    "            ):\n",
    "                raise ValueError(\"input_features ≠ feature_names_in_\")\n",
    "            return input_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
